#!/usr/bin/env python

'''
Filename: blob_reader.py

Description: Reads the .blobs files generated by the Multi-Worm Tracker
program. Calculates basic properties for each blob and determines wheather
it matches criterion for further processing.
'''

__author__ = 'Peter B. Winter'
__email__ = 'peterwinteriii@gmail.com'
__status__ = 'prototype'

# standard imports
import numpy as np
import math
import glob
from itertools import izip
from Encoding.decode_outline import decode_outline

# TODO. move this into the testing dir.
def sample_function(ex_dir):
    sfile = glob.glob(ex_dir + '*.summary')[0]
    blob_files = sorted(glob.glob(ex_dir + '*.blobs'))
    blob_durations, blob_locations, blob_starts, blob_ends = read_summary_events(sfile)
    
    # check if blob numbers match the correct files.
    '''
    falsely_identified = []
    for b, (f_num, bit_offset) in blob_locations.iteritems():
        if b not in bfile_blobs[f_num]:
            falsely_identified.append(b)
    print len(falsely_identified), 'wrong out of', len(blob_locations)            
    '''
    min_time = 120
    blobs_to_check = [b for (b, dur) in blob_durations.iteritems() if dur >= min_time]
    for b in blobs_to_check[:]:
        blob_lines = return_blob_lines(blob_id=b, blob_locations=blob_locations, file_index=blob_files)

class Blob_Reader(object):
    """
    """
    
    def __init__(self, path, min_body_lengths=2, min_duration=120, min_size=50):
        """
        """
        if path[-1] != '/':
            path += '/'

        self.min_body_lengths = min_body_lengths
        self.min_duration = min_duration
        self.min_size = min_size
        self.blob_files = sorted(glob.glob(path + '*.blobs'))        
        self.sfile = glob.glob(path + '*.summary')[0]
        #TODO: write a check to make sure a summary file is present. 
        
        blob_durations, blob_locations, blob_starts, blob_ends = self.read_summary_events(self.sfile)
        self.blob_durations = blob_durations
        self.blob_locations = blob_locations
        self.blob_starts = blob_starts
        self.blob_ends = blob_ends
        self.blobs_to_check = [b for (b, dur) in blob_durations.iteritems() if dur >= min_duration]
        #print len(self.blobs_to_check), self.blobs_to_check
        #TODO: write a check to make sure same number of blob files present as in blob_locations
        # make sure blobs files are present and none missing.
        
    def pull_worthy_blobs(self, verbose=False):        
        def make_temp_blob():
            return {'time': [], 'xy': [], 'aspect_ratio': [],
                     'size': [], 'outline': [], 'midline': []}

        def make_local_id(local_id):
            # local blob ids in the database are always length 5.
            #if this one is short, pad front with '0's
            for i in range(5):
                if len(local_id) < 5:
                    local_id = '0' + local_id
            return local_id

        def process_blob(temp_blob, blobs, local_id):
            # to save calculation time, first check if blob meets duration requirement
            if len(temp_blob['time']) > 0:
                duration = temp_blob['time'][-1] - temp_blob['time'][0]
            else:
                duration = 0
            if duration >= self.min_duration:
                aggregate_attributes = compute_basic_properties(temp_blob)
                if is_blob_worthy(aggregate_attributes, self.min_body_lengths,
                                    self.min_duration, self.min_size):
                    temp_blob['attributes'] = aggregate_attributes
                    blobs[local_id] = temp_blob
            return blobs

        blobs = {}
        total_count =0
        N = len(self.blobs_to_check)
        for i, bID in enumerate(self.blobs_to_check):
            if verbose:
                print 'checking {bID} ({i}/{N})'.format(bID=bID, i=i, N=N)
            local_id = make_local_id(str(bID))
            temp_blob = make_temp_blob()
            for line in self.return_blob_lines(blob_id=bID):
                line = line.strip('\n\r')
                cols = line.split()
                # skip empty lines
                if len(cols) == 0:
                    continue
                #time_key = str(cols[1]).replace('.', '?')
                temp_blob['time'].append(float(cols[1]))
                temp_blob['xy'].append((float(cols[2]), float(cols[3])))
                temp_blob['size'].append(int(cols[4]))
                try:
                    aspect_ratio = float(cols[8]) / float(cols[9])
                except: 
                    # add in a -1 if the ratio gets too big.
                    aspect_ratio = -1.0
                temp_blob['aspect_ratio'].append(aspect_ratio)
                outline = parse_outline_from_line(line)
                if outline == None: 
                    outline = ['', '', '', '']
                temp_blob['outline'].append(outline)
                midline = calculate_midline_from_line(line)
                if midline == None: 
                    midline = ''
                temp_blob['midline'].append(midline)
            # need to check if last blob should be saved.
            blobs = process_blob(temp_blob, blobs, local_id)
        return blobs
         
    def read_summary_events(self, sfile):
        """ returns two dictionaries in which blob_ids are keys, blob_durations and blob_locationss.
    blob_durations contains the number of seconds each blob was tracked.
    blob_locationss contains the file number and bit offset inside that file in which the blobs data is stored.
    """

        def parse_line_segment(line_segment):
            # line segments ususally contain an unspecified number of paired values.
            # this parses the paired values and returns them as two lists, part_a and part_b        
            assert isinstance(line_segment, str)
            part_a, part_b = [], []
            cols = line_segment.split()
            assert len(cols) % 2 == 0
            it = iter(cols)
            for a, b in izip(it, it):
                part_a.append(a)
                part_b.append(b)
            return part_a, part_b

        blob_locations = {}
        blob_starts = {}
        blob_ends = {}

        with open(sfile, 'r') as f:
            lines = f.readlines()
            for line in lines[:]:
                # store all blob locations and remove them from end of line.
                splitline = line.split('%%%')
                if len(splitline) == 2:
                    line, file_stuff = splitline
                    blobs, locations = parse_line_segment(file_stuff)
                    new_blob_locationss = {}
                    for b, l in izip(blobs, locations):
                        new_blob_locationss[int(b)] = map(int, l.split('.'))
                    blob_locations.update(new_blob_locationss)
                # store all blob start and end times and remove them from end of line.
                splitline = line.split('%%')
                if len(splitline) == 2:
                    line, blob_stuff = splitline                
                    lost_blobs, found_blobs = parse_line_segment(blob_stuff)
                    time = line.split()[1]
                    for b in lost_blobs:
                        blob_ends[int(b)] = time
                    for b in found_blobs:
                        blob_starts[int(b)] = time

        # use start and end times to calculate all durations.
        blob_durations = {}            
        for b, start in blob_starts.iteritems():
            # if end time not stored, blob existed until the end of the recording.
            end = blob_ends.get(b, None)
            if not end:
                blob_ends[b] = time
                end = time
            blob_durations[int(b)] = float(end) - float(start)        
        return blob_durations, blob_locations, blob_starts, blob_ends

    def return_blob_lines(self, blob_id):
        file_no, bit_offset = self.blob_locations.get(blob_id, (None, None))
        with open(self.blob_files[file_no], 'r') as f:
            #help(f)
            f.seek(bit_offset)
            line_id = None
            #first_time, time = 0, 0
            times = []
            lines = []
            for line in f:
                # ID line
                if line[0] == '%':
                    # if line_id, then data for a new blob is starting. break.
                    if line_id:
                        break
                    # if line_id not defined, this is the first line looked at. ensure it matches blob_id.
                    else:
                        line_id = int(line[1:])
                        if line_id != blob_id:
                            print 'WARNING index/blobs file mismatch'
                            print 'blob id:', blob_id
                            print 'line id:', line_id
                            return []
                # data line, store for processing
                else:
                    lines.append(line)
        return lines

def parse_outline_from_line(line):
    ''' accepts a single line from a blobs file and returns a parsed outline 
    or None.
    '''    
    # the outline portion of the file starts with '%%' and goes to end of line
    split_line = line.split('%%')
    if len(split_line) != 2: return None
    cols = split_line[-1].split()
    # there are 4 columns (int, int, int, string)
    # (x, y, length of outline, outline encoded in string)
    if len(cols) != 4: return None
    try:
        x, y = int(cols[0]), int(cols[1])
        p_num = int(cols[2])
        outline_code = cols[3]
        return x, y, p_num, outline_code
    except Exception as e:
        print e
        return None

def calculate_midline_from_line(line):
    ''' accepts a single line from a blobs file and returns a length estimate 
    or None.
    '''
    # the midline portion starts with a '%' and ends either with '%%' or line end.
    split_line = line.split('%') 
    if len(split_line) < 2: return None
    if len(split_line) > 4: return None
    cols = split_line[1].split()
    # there are 22 columns (all integers)
    if len(cols) != 22: return None
    # [x1, y1, x2, y2, ... ]
    try: 
        xs = [float(x) for x in cols[::2]]
        ys = [float(y) for y in cols[1::2]]
        midline = 0
        for i in range(len(xs)-1):
            midline += math.sqrt((ys[i] - ys[i+1])**2 + (xs[i] - xs[i+1])**2)
        return midline
    except Exception as e:
        print e
        return None
    
def calculate_midline_from_line2(line):
    ''' accepts a single line from a blobs file and returns a length estimate 
    or None.
    '''
    # the midline portion starts with a '%' and ends either with '%%' or line end.
    split_line = line.split('%') 
    if len(split_line) < 2: return None
    if len(split_line) > 4: return None
    cols = split_line[1].split()
    # there are 22 columns (all integers)
    if len(cols) != 22: return None
    # [x1, y1, x2, y2, ... ]
    try: 
        dx = np.diff(np.array([float(x) for x in cols[::2]]))
        dy = np.diff(np.array([float(y) for y in cols[1::2]]))
        return np.sqrt(dx**2 +dy**2)
    except Exception as e:
        print e
        return None

def is_blob_worthy(aggregate_attributes, min_body_lengths, min_duration, min_size): 
    ''' accepts dictionary of aggregate_attributes and returns True if it passes
    all tests and False if it fails any. Tests are specified by the following inputs
    min_body_lengths -- distance blob must travel (in body lengths)
    min_duration -- seconds blob must be tracked
    min_size -- number of pixels blob must contain.
    '''
    # reject blob immediatly if it fails any test
    assert type(aggregate_attributes) == dict
    if len(aggregate_attributes) == 0: return False
    for a in ['size_median', 'duration', 'bl_dist']:
        if a not in aggregate_attributes: return False
    if aggregate_attributes['size_median'] < min_size: return False
    if aggregate_attributes['duration'] < min_duration: return False
    if aggregate_attributes['bl_dist'] < min_body_lengths: return False
    return True

def compute_basic_properties(blob, distance_metric='box_diagonal'):
    '''
    blob -- a dictionary containing several time series that are sometimes lists
            and sometimes dicts.
    distance_metric -- calculates the distance a blob travels differently
            options:
            'box_diagonal' -- (default) draws a box around full path (ie finds max and min
                xy coordinates) and measures distance as the diagonal between corners. used
                because it is robust to minor vibrations.
            'full_path' -- distance is calculated using the sum of all the distances
              between consecutive points. not robust to vibrations.
            'start_to_end' -- distance is just distance between first and last points.
    '''
    # dont calculate anything if basic properies not met
    assert type(blob) == dict
    if len(blob) == 0: return {}
    for a in ['time', 'xy', 'size', 'midline']:         
        if a not in blob: return {}
        if len(blob[a]) == 0: return {}
    assert isinstance(blob['time'], list)
    assert isinstance(blob['xy'], list)
    assert isinstance(blob['size'], list)
    assert isinstance(blob['midline'], list)
    assert distance_metric in ['box_diagonal', 'full_path', 'start_to_end']
    # calculate properties from size
    times = blob['time']
    median_size = np.median(blob['size'])
    #print blob['midline']
    #print set([type(m) for m in blob['midline']])
    midlines = [m for m in blob['midline'] if type(m) ==float]
    median_midline = np.median(midlines)

    if distance_metric == 'start_to_end':
        d = 0
        for i in range(len(blob['xy']) - 1):
            x1, y1 = blob['xy'][i]
            x2, y2 = blob['xy'][i+1]
            d += math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)
    elif distance_metric == 'full_path':
        x1, y1 = blob['xy'][0]
        x2, y2 = blob['xy'][-1]
        d = math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)
    elif distance_metric == 'box_diagonal':
        x, y = zip(*blob['xy'])
        d = math.sqrt((max(x) - min(x)) ** 2 + (max(y) - min(y)) ** 2)

    bl_dist = d / median_midline
    timeseries_properties = {'size_median': median_size,
                             'midline_median': median_midline,
                             'start_time': times[0],
                             'stop_time': times[-1],
                             'duration': times[-1] - times[0],
                             'bl_dist': bl_dist}
    return timeseries_properties


def main():
    import time



    #help(time)
    data_dir = '/home/projects/worm_movement/Data/MWT_RawData/'
    #print glob.glob(data_dir + '*')
    ex_id = '20130621_121947'
    #ex_id = '20130318_153749'
    print ex_id
    path = data_dir + ex_id + '/'
    bfiles = glob.glob(path + '*.blobs')

    start_time2 = time.time()
    BR = Blob_Reader(path)
    blobs_method2 = BR.pull_worthy_blobs()
    t2 = time.time() - start_time2
    b2 = blobs_method2.keys()
    print 'time2', t2
    print 'len', len(b2), b2

    start_time1 = time.time()
    blobs_method1 = {}
    for bfile in bfiles:
        print bfile
        blobs_method1.update(pull_worthy_blobs_from_blobs_file(bfile, min_body_lengths=2, min_duration=120, min_size=50))
    
    t1 = time.time() - start_time1
    b1 = blobs_method1.keys()
    print 'time1', t1
    print 'len', len(b1), b1


    print 't2 is ', t1 / t2, 'times faster'
    for b in b1:
        if b not in b2:
            print 'did not find', b
    for b in b2:
        if b not in b1:
            print 'found extra', b

    

if __name__ == '__main__':
    main()
